--- FILE: CallsignLookupAlgorithm.md ---
# Callsign Lookup Algorithm Specification

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document describes the algorithm implemented in `get_cty.py` for determining an amateur radio callsign's DXCC entity and other geographical data based on the `CTY.DAT` file.

---
### --- Revision History ---
- 2025-08-03: Replaced the 'Final Fallback' heuristic with a rule to return 'Unknown' for ambiguous calls and inserted the 'ends in a digit' heuristic.
- 2025-08-02: Added `portableid` to the output data structure and a rule to invalidate `digit/call` formats.
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's goal is to replicate the logic of major contest logging programs by implementing a precise, ordered, multi-step algorithm. It takes a raw callsign string as input and returns a data tuple containing the resolved DXCC entity, CQ/ITU Zones, continent, and a new `portableid` field.

## 2. Output Data Structure

The script's output is a `FullCtyInfo` named tuple, which has been modified to include the `portableid` field.

`(DXCCName, DXCCPfx, CQZone, ITUZone, Continent, Lat, Lon, Tzone, WAEName, WAEPfx, portableid)`

The `portableid` field will contain the specific part of a portable callsign that was used to determine the location (e.g., "7", "VP2V") and will be blank for non-portable callsigns.

## 3. The Lookup Algorithm

The script follows a strict order of operations. A successful match at any step concludes the algorithm.

### Step 1: Pre-processing (`_preprocess_callsign`)
The initial step is to clean the raw callsign string to create a standardized base for analysis. This involves stripping common non-prefix suffixes such as `/P`, `/M`, `/QRP`, `/B`, and any characters following a hyphen (`-`).

### Step 2: Exact Match (`_check_exact_match`)
The highest-priority lookup is for an exact match. The `CTY.DAT` file can contain entries prefixed with `=` that map a full, unique callsign to a specific entity. The script checks for these first.

### Step 3: Hardcoded Special Cases (`_check_special_cases`)
The script then checks for hardcoded exceptions that do not follow standard patterns. The primary rules are:
- Any callsign ending in `/MM` (Maritime Mobile) is immediately classified as an "Unknown" entity.
- A specific rule correctly identifies `KG4` callsigns as Guantanamo Bay.

### Step 4: Portable Call Logic (`_handle_portable_call`)
If the cleaned callsign contains a `/`, it is processed by a dedicated handler that uses a series of heuristics to identify the `portableid`. See Section 4 for details.

### Step 5: Longest Prefix Match (`_find_longest_prefix`)
If the call is not resolved by any of the previous steps, this default lookup method is used. It takes the callsign string (e.g., `VP2VMM`) and checks if it is a known prefix. If not, it removes the last character and tries again (`VP2VM`), repeating this process until it finds the longest possible valid prefix (`VP2V`) that exists in the `CTY.DAT` data.

## 4. Portable Call Heuristics

The `_handle_portable_call` method uses the following ordered checks. If a rule is satisfied, a result is returned and the process stops.

1.  **Invalid `digit/call` Format:** The script first checks for the invalid `digit/callsign` format (e.g., `7/KD4D`). If this pattern is found, the call is considered invalid and returns "Unknown".

2.  **Unambiguous Prefix Rule:** The script checks if exactly one side of the `/` is a valid prefix in `cty.dat` while the other is not. If so, the valid side is identified as the `portableid`.

3.  **"Strip the Digit" Heuristic:** If the call is still ambiguous, this tie-breaker temporarily strips a trailing digit from each side. If this makes one side a valid prefix while the other remains invalid, the original, unmodified side that produced the match is chosen as the `portableid`. This is critical for calls like `HC8N/4`.

4.  **US/Canada Heuristic:** This rule handles the `callsign/digit` format for domestic US/Canada calls. If one side appears to have the structure of a US or Canadian callsign and the other is a single digit, the script identifies the **single digit** as the `portableid`.

5.  **"Ends in a Digit" Heuristic:** This is the final tie-breaker. If exactly one side of the `/` ends in a digit while the other does not, the side ending in the digit is identified as the `portableid`. This correctly resolves calls like `WT7/OL5Y`.

6.  **Final Action (No Fallback):** If a call remains ambiguous after all of the above heuristics have been attempted, the script **gives up and returns "Unknown"**. The previous "Final Fallback" logic that attempted to guess the location has been removed to prevent incorrect resolutions.
--- FILE: InstallationGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/InstallationGuide.md

# Installation Guide - Contest Log Analyzer

**Version: 0.26.2-Beta**

This guide provides step-by-step instructions to set up the Contest Log Analyzer on your system. The process involves installing a Python distribution, setting up a dedicated environment, installing the required libraries, and configuring the necessary data files.

---

### Step 1: Prerequisites

Before you begin, you will need to have **Git** installed on your system. Git is a version control system used to download the project's source code. You can download it from [git-scm.com](https://git-scm.com/).

---

### Step 2: Download the Project Files

Open your command prompt or terminal and navigate to the directory where you want to store the project (e.g., your Desktop). Then, run the following command to download the project from its GitHub repository:
--- FILE: ProgrammersGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/ProgrammersGuide.md

# Contest Log Analyzer - Programmer's Guide

**Version: 0.26.2-Beta**

## 1. Introduction

This guide is for developers who want to understand, maintain, or extend the Contest Log Analyzer. It provides an overview of the project's architecture, explains the core data flow, and gives step-by-step instructions for common development tasks like adding new reports, contest definitions, and scoring logic.

---

## 2. Project Architecture & Directory Structure

The project is designed to be highly modular, separating data, processing, and presentation.

* `Contest-Log-Analyzer/` (Project Root)
    * `main_cli.py`: The main command-line interface (CLI) script and the entry point for the application.
    * `contest_tools/`: The core Python application package containing all the processing logic.
        * `cabrillo_parser.py`: Contains low-level functions for reading and parsing the standard Cabrillo log file format.
        * `contest_log.py`: Defines the `ContestLog` class, the central object that holds all data and metadata for a single, fully processed log.
        * `log_manager.py`: Defines the `LogManager` class, which handles loading and managing one or more `ContestLog` instances for comparative analysis.
        * `report_generator.py`: Defines the `ReportGenerator` class, which orchestrates the execution of all reports.
        * `contest_definitions/`: A data-driven package containing JSON files that define the rules, multipliers, and exchange formats for each supported contest.
        * `core_annotations/`: Contains modules for universal data enrichment that applies to most contests, such as country lookup (`get_cty.py`) and Run/S&P classification (`run_s_p.py`).
        * `contest_specific_annotations/`: Contains modules with logic unique to a specific contest, such as the scoring rules for CQ WW or ARRL SS.
        * `reports/`: The "plug-and-play" reporting system. Each Python file in this directory is a self-contained report generator that is automatically discovered by the program.
    * `Logs/` (Recommended User Directory)
        * This directory is the recommended location for storing your raw Cabrillo log files, organized by year and contest.
    * `reports_output/` (Generated Directory)
        * This directory is automatically created by the program to store all generated reports and charts.

---

## 3. Core Concepts & Data Flow

The process follows a clear pipeline:

1.  **Loading**: The `LogManager` is called by `main_cli.py` with paths to raw Cabrillo logs.
2.  **Definition**: The manager reads the `CONTEST:` header from each file to identify the contest (e.g., "CQ-WW-CW"). It then loads the corresponding JSON file (e.g., `cq_ww_cw.json`) into a `ContestDefinition` object.
3.  **Parsing**: The `cabrillo_parser` uses the rules from the `ContestDefinition` object to parse the raw text file into a structured pandas DataFrame and a metadata dictionary.
4.  **Instantiation**: A `ContestLog` object is created to hold the parsed DataFrame and metadata for each log.
5.  **Annotation**: The `ContestLog` object's `apply_annotations()` method is called. This is a crucial step where the raw data is enriched:
    * Core Annotations are applied first (Country/Zone lookup, Run/S&P classification).
    * Contest-Specific Annotations are applied next (QSO point calculation, multiplier identification).
6.  **Reporting**: The final, fully-annotated list of `ContestLog` objects is passed to the `ReportGenerator`, which executes the requested reports based on their defined capabilities (single, pairwise, multi-log).

---

## 4. Extending the Analyzer

### How to Add a New Report

The reporting system is designed to be "plug-and-play." To add a new report, you simply create a new Python file in the `contest_tools/reports/` directory. The system will automatically discover it.

#### The Report Interface

Every report file must contain a class named `Report` that inherits from `ContestReport`. This base class ensures a consistent structure. You must define the following class attributes:

* `report_id`: A unique, machine-readable string for your report (e.g., `band_summary`). This is what the user types on the command line.
* `report_name`: A human-readable name for your report (e.g., "QSOs per Band Summary").
* `report_type`: Must be one of `text`, `plot`, or `chart`. This determines the output subdirectory.
* `supports_single`, `supports_pairwise`, `supports_multi`: Booleans (`True`/`False`) that tell the `ReportGenerator` how to run your report.

You must also implement the `generate(self, output_path: str, **kwargs)` method. This is where your main logic goes. It is responsible for saving its own output file(s) and must return a string confirmation message.

#### Step-by-Step Guide

1.  **Create Your Report File**: In the `contest_tools/reports/` directory, create a new Python file (e.g., `text_my_new_report.py`).
2.  **Use a Template**: Copy the contents of an existing report file (like `text_summary.py`) into your new file to get the correct structure.
3.  **Customize Your Report Class**:
    * Update the class attributes (`report_id`, `report_name`, etc.).
    * Write your analysis logic in the `generate` method.
        * Access the fully processed logs via `self.logs`.
        * Get the pandas DataFrame with `log.get_processed_data()`.
        * Get the header data with `log.get_metadata()`.
        * Safely access optional arguments via `kwargs.get('arg_name', default_value)`.
4.  **Run It!** The system will automatically discover your report.

### How to Add a New Contest Definition

If you want to add support for a contest not currently defined, you only need to create a new JSON file.

1.  **Create JSON File**: In `contest_tools/contest_definitions/`, create a new file (e.g., `arrl_dx_cw.json`).
2.  **Define `contest_name`**: Add the exact name from the Cabrillo `CONTEST:` header (e.g., `"contest_name": "ARRL-DX-CW",`).
3.  **Define Exchange Parsing**: Under `exchange_parsing_rules`, create an entry for the contest name. Provide a regex to capture the exchange fields and a list of `groups` to name them.
4.  **Define Multipliers**: Add a `multiplier_rules` list to define the multipliers for the contest (e.g., states, countries). Specify the source of the multiplier data.
5.  **(Optional) Add Scoring**: If the contest requires custom scoring, see the next section.

### How to Add New Contest-Specific Scoring

The system can dynamically load scoring logic for any contest.

1.  **Create Scoring File**: In `contest_tools/contest_specific_annotations/`, create a new Python file whose name matches the contest's JSON file (e.g., `arrl_dx_scoring.py`).
2.  **Implement `calculate_points`**: The file must contain a function with the signature `calculate_points(df: pd.DataFrame, my_call_info: Dict[str, Any]) -> pd.Series`.
3.  **Write Logic**: Inside this function, write the logic to calculate the point value for each QSO in the input DataFrame (`df`). The `my_call_info` dictionary provides the operator's own location data, which is often needed for scoring.
4.  **Automatic Discovery**: The `ContestLog` class will automatically find and execute this function during the annotation process based on the contest name.
--- FILE: README.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/README.md

# Contest Log Analyzer

**Version: 0.26.2-Beta**

A Python-based tool for in-depth analysis and comparison of amateur radio contest logs. This application processes standard Cabrillo files to generate detailed reports, charts, and visualizations, providing deep insights into operator strategy and performance.

## Key Features

* **Data-Driven Architecture**: Uses simple JSON files to define the rules, scoring, and exchange formats for each contest, making the tool highly extensible.
* **Advanced Log Processing**: Ingests standard Cabrillo files and enriches the data with universal annotations (DXCC, WAE, Continent, Zones) and contest-specific data.
* **Run/S&P/Unknown Classification**: A sophisticated, multi-pass heuristic analyzes QSO patterns to classify each contact as "Run," "Search & Pounce," or "Unknown," providing a clear picture of operating strategy.
* **Contest-Specific Scoring**: A modular system calculates QSO points based on the official rules for supported contests (ARRL-DX, ARRL-SS, CQ-WPX, CQ-WW).
* **Dynamic Reporting Engine**: A flexible, "plug-and-play" system for generating a wide variety of text, plot, and chart-based reports.
* **Comparative Analysis**: Most reports are designed for pairwise or multi-log comparison, allowing you to analyze your performance against others.

## Installation

This project uses `conda` for environment and package management.

#### 1. Clone the Repository

```
git clone [https://github.com/kd4d/Contest-Log-Analyzer.git](https://github.com/kd4d/Contest-Log-Analyzer.git) "Contest-Log-Analyzer"
cd "Contest-Log-Analyzer"
```

#### 2. Create and Activate Conda Environment

It is recommended to use Miniforge and create a dedicated environment.

```
conda create --name contest-analyzer python=3.11 -y
conda activate contest-analyzer
```

#### 3. Install Dependencies

Update the base packages and then install the required libraries.

```
conda update --all -y
conda install pandas matplotlib seaborn -y
```

#### 4. Set Up Environment Variable

The program requires the `CONTEST_DATA_DIR` environment variable to be set to the location of your data directory.

* **Windows (Temporary):**
    ```
    set CONTEST_DATA_DIR="C:\path\to\your\Contest-Log-Analyzer\data"
    ```
* **macOS/Linux (Temporary):**
    ```
    export CONTEST_DATA_DIR="/path/to/your/Contest-Log-Analyzer/data"
    ```

#### 5. Download Data Files

Place the necessary data files in a central `data/` directory.

* **Required for all contests:** `cty.dat` (from [country-files.com](http://www.country-files.com/cty/cty.dat))
* **Required for ARRL DX:** `ARRLDXmults.dat`
* **Required for ARRL SS:** `SweepstakesSections.dat`

## Usage

The analyzer is run from the command line using `main_cli.py`.

#### **Basic Syntax**

```
python main_cli.py --report <ReportID|all> <LogFile1> [<LogFile2>...] [options]
```

#### **Examples**

* **Generate all available reports for two logs:**
    ```
    python main_cli.py --report all Logs/2024/cq-ww-cw/k3lr.log Logs/2024/cq-ww-cw/kc1xx.log
    ```
* **Generate a specific report (Score Summary) for a single log:**
    ```
    python main_cli.py --report score_report Logs/2024/cq-ww-cw/k3lr.log
    ```
* **Generate a Missed Multipliers report for CQ WW Zones:**
    ```
    python main_cli.py --report missed_multipliers --mult-name Zones Logs/2024/cq-ww-cw/k3lr.log Logs/2024/cq-ww-cw/kc1xx.log
    ```

## Available Reports

All generated files are saved to a structured directory under `reports_output/YYYY/CONTEST_NAME/`.

#### **Text Reports (`text/`)**

* `summary`: High-level overview of QSO counts (Run, S&P, Unknown).
* `score_report`: Comprehensive score breakdown by band for a single log.
* `rate_sheet`: Detailed hourly QSO rates per band for a single log.
* `rate_sheet_comparison`: Side-by-side hourly rate comparison for multiple logs.
* `qso_comparison`: Detailed pairwise breakdown of Total, Unique, and Common QSOs.
* `missed_multipliers`: Comparative report showing multipliers missed by each station.
* `multiplier_summary`: Detailed breakdown of QSOs per multiplier.
* `multipliers_by_hour`: Shows new multipliers worked each hour of the contest.
* `continent_summary`: Total QSOs per continent for a single log.
* `comparative_continent_summary`: Side-by-side comparison of QSOs per continent.
* `continent_breakdown`: Detailed QSOs per continent broken down by Run/S&P status.

#### **Plots (`plots/`)**

* `qso_rate_plots`: Cumulative QSO rate line graphs.
* `point_rate_plots`: Cumulative point rate line graphs.
* `cumulative_difference_plots`: Plot showing the running QSO or Point difference between two logs.

#### **Charts (`charts/`)**

* `qso_breakdown_chart`: Stacked bar chart comparing unique QSO counts for two logs.
* `chart_point_contribution`: Side-by-side pie charts comparing point sources.
* `chart_point_contribution_single`: Per-band pie charts showing point sources for one log.

## License

This project is licensed under the **Mozilla Public License, v. 2.0**.

--- FILE: ReportInterpretationGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/ReportInterpretationGuide.md

# Report Interpretation Guide

**Version: 0.26.3-Beta**

## 1. Introduction

This guide explains how to read and interpret the various reports generated by the Contest Log Analyzer. The goal is not just to understand the numbers, but to turn them into actionable insights that can help you improve your scores in future contests.

All examples in this guide are from the **2024 CQ WW DX CW Contest**, comparing the logs from K1LZ, K3LR, and KC1XX.

---

## 2. Text Reports

Text reports provide detailed, granular data in a plain-text format. They are best for deep-dive analysis and finding specific details.

### Score Report (`score_report`)

This is a comprehensive, single-log summary of your final score, broken down by band. It provides the most important top-level metrics for your operation.

#### Example Output

<!-- Text from score_report_K3LR.txt will be inserted here. -->

#### How to Interpret This Report

* **AVG (Average Points per QSO)**: This is a crucial metric. An AVG close to 3.0 indicates a strong focus on high-value inter-continental QSOs. An AVG closer to 1.0 or 2.0 suggests more contacts within your own continent.
* **Multiplier Totals**: For contests like CQ WW, multipliers are counted on each band. The `TOTAL` row shows the *sum* of multipliers from each band, not the number of unique multipliers.

---

### QSO Summary (`summary`)

This report provides a high-level comparative overview of the total QSO counts and operating styles for all analyzed logs.

#### Example Output

<!-- Text from summary_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

This report is the quickest way to understand fundamental differences in operating strategy. In this example, all three stations have a very similar **Run-to-S&P ratio** (roughly 2:1), indicating a shared, effective strategy focused on calling CQ ("Running") as much as possible.

---

### Rate Sheet (`rate_sheet` & `rate_sheet_comparison`)

The `rate_sheet` provides an hour-by-hour breakdown of QSO rates for a single log, while the `rate_sheet_comparison` places this data side-by-side for multiple logs.

#### Example Output (Comparison)

<!-- Text from rate_sheet_comparison_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

Use this report to analyze strategic differences in band selection and pacing. At `0000Z`, K1LZ focused heavily on 40M, while K3LR was already making significant numbers on 10M, indicating a different approach to chasing band openings.

---

### QSO Comparison Summary (`qso_comparison`)

This powerful pairwise report breaks down how two logs compare on each band, focusing on which QSOs were **unique** to each station and which were **common** to both.

#### Example Output (10 Meter Band)

<!-- Text from qso_comparison_K3LR_vs_K1LZ.txt will be inserted here. -->

#### How to Interpret This Report

The **Unique** columns reveal the real strategy. K3LR's 878 unique QSOs came primarily from "Running" (745 QSOs). This means K3LR's high rate on 10M was effective at attracting stations that K1LZ never logged. A high number of "Unique S&P" QSOs would indicate that one operator was more effective at searching for and finding rare stations that the other missed.

---

### Missed Multipliers (`missed_multipliers`)

This report is essential for identifying the most costly unworked stations. It lists every multiplier that was worked by at least one person in the group but missed by at least one other, showing who worked it and how.

#### Example Output (10 Meter Band Snippet)

<!-- Text from missed_multipliers_countries_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

The text `(Run)`, `(S&P)`, or `(Both)` shows how the station logged that multiplier. This is critical information. If your competitor worked a rare multiplier via `(S&P)`, it means they found it by tuning the band. If they got it via `(Run)`, it means that multiplier *called them*.

---

## 3. Plots and Charts

Plots and charts provide a high-level, visual summary of performance.

### QSO Rate Plot (`qso_rate_plots`)

This plot shows the cumulative QSO total over the course of the contest. It's the best way to visualize the overall "horse race."

<!-- There will be an image/file inserted here: qso_rate_all_plot.png -->

#### How to Interpret This Plot

* **Slope of the Line**: A steeper slope indicates a higher QSO rate. You can see periods where one station pulls ahead or another catches up.
* **Plateaus**: Flat sections of the graph indicate off-times or periods of very low activity.
* **Inset Table**: The table provides a convenient summary of the final QSO totals and the Run/S&P/Unknown breakdown for each station.

---

### Cumulative Difference Plot (`cumulative_difference_plots`)

This unique plot is one of the most powerful analysis tools in the package. It visualizes the *difference* between two logs over time, broken down by operating style.

<!-- There will be an image/file inserted here: diff_qsos_all_K3LR_vs_KC1XX.png -->

#### How to Interpret This Plot

This plot shows **K3LR minus KC1XX**.

* **Top Panel (Overall Diff)**: This shows the total QSO difference. When the line is above zero, K3LR is ahead. When it drops below zero, KC1XX has taken the lead.
* **Middle Panel (Run Diff)**: This isolates the difference in **Run** QSOs. K3LR's initial lead was built on a superior run rate, but the downward trend shows KC1XX consistently out-ran K3LR after the first few hours.
* **Bottom Panel (S&P+Unk Diff)**: This isolates the difference in **S&P** QSOs. The upward trend shows K3LR was significantly more effective at Search & Pounce.
* **Strategic Insight**: This plot tells a clear story. K3LR's S&P strategy was superior, but it was not enough to overcome the deficit from KC1XX's more powerful run rate in the second half of the contest.

---

### QSO Breakdown Chart (`qso_breakdown_chart`)

This chart provides a visual companion to the `qso_comparison` text report, breaking down the unique and common QSOs on each band.

<!-- There will be an image/file inserted here: qso_breakdown_chart_K3LR_vs_K1LZ.png -->

#### How to Interpret This Chart

This chart is unique to this analyzer and reveals where each station gained its advantage.

* **Gray Bar (Common)**: This represents the base of QSOs that both stations worked.
* **Colored Bars (Unique)**: The stacked, colored bars on either side show the QSOs that were unique to that station, broken down by Run, S&P, and Unknown.
* **Strategic Insight**: On 20 meters, the high "Common" bar shows both stations worked a similar pool of stations. However, both added a significant number of **Unique Run** QSOs (the large red segments), reinforcing that they were successfully running different sets of stations.

---

### Point Contribution Breakdown (`chart_point_contribution`)

This chart shows where your points came from. For CQ WW, points are awarded based on the continent of the station worked.

<!-- There will be an image/file inserted here: chart_point_contribution_all_bands_K1LZ_vs_K3LR_vs_KC1XX.png -->

#### How to Interpret This Chart

* **Pie Chart**: The slices show the proportion of total points that came from each point value. In CQ WW, 3-point inter-continental QSOs are the most valuable.
* **Table**: The table below the chart provides the exact counts for each point category and the final average points per QSO.
--- FILE: RunS&PAlgorithm.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/RunS&PAlgorithm.md

# Run/S&P/Unknown Classification Algorithm

**Version: 0.26.2-Beta**

The purpose of this algorithm is to analyze a contest log and infer the operator's activity for each contact, classifying it as one of three types: **Run**, **Search & Pounce (S&P)**, or **Unknown**.

The analysis is performed independently for each operating "stream"—a unique combination of a band and mode (e.g., 20M CW is one stream, 40M SSB is another). The algorithm uses a two-pass approach.

---

### Pass 1: Initial Classification (Run vs. S&P)

The first pass uses a "sticky run" state machine to make an initial classification.

1.  **Identifying a Run**: A "run" is defined as a period of high-rate activity on a single frequency. The algorithm identifies the start of a run when it detects a minimum number of QSOs (typically 3) occurring on the same frequency within a short time window (e.g., 10 minutes).
2.  **The "Sticky" State**: Once a run is identified, the algorithm enters a "run state." It assumes the operator is still running and will continue to classify all subsequent QSOs on that frequency as **Run**.
3.  **Breaking a Run**: The run state is maintained until one of two conditions is met:
    * **Time-Out**: A significant amount of time (e.g., 2 minutes) passes without a QSO on the run frequency.
    * **Frequency Change**: The operator makes several consecutive QSOs on other frequencies, indicating they have moved to search for new contacts.
4.  **S&P Classification**: Any QSO that is not part of an identified run is initially classified as **S&P**.

---

### Pass 2: Reclassification of Low-Rate QSOs

The second pass refines the results by identifying periods of very low activity where the operator's intent is ambiguous.

1.  **Reviewing S&P QSOs**: The algorithm re-examines only the QSOs that were classified as **S&P** in the first pass.
2.  **Checking the Rate**: For each S&P QSO, it looks at the number of other contacts made in a time window both before and after it (e.g., 15 minutes).
3.  **Reclassifying to Unknown**: If the QSO rate in the surrounding period is below a certain threshold, the activity is considered too low to be definitively classified. In this case, the QSO's status is changed from S&P to **Unknown**.

The final output is a log where every contact is annotated with its inferred operating style: **Run**, **S&P**, or **Unknown**.
--- FILE: UsersGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/UsersGuide.md

# Contest Log Analyzer - User Guide

**Version: 0.26.2-Beta**

## 1. Introduction: What is the Contest Log Analyzer?

The Contest Log Analyzer is a powerful command-line tool designed for amateur radio contesters who want to perform deep, data-driven analysis of their operating performance. It goes beyond the simple score summary provided by most logging software, allowing you to:

* Process raw Cabrillo log files into a clean, standardized format.
* Automatically classify every QSO as "Run," "Search & Pounce," or "Unknown" to analyze your operating strategy.
* Generate detailed reports and charts that compare your log against one or more others.
* Analyze performance on a band-by-band basis to identify strengths and weaknesses.
* Calculate contest-specific QSO points for supported contests.

The ultimate goal of this program is to help you understand your contest operation in minute detail, identify missed opportunities, and improve your strategy for the next event.

---

## 2. What You Need to Get Started

Before running the analyzer, you will need a few files:

* **Your Cabrillo Log File(s)**: These are the standard log files generated by your contest logging software (e.g., `k3lr.log`, `kc1xx.log`). You can analyze a single log or compare multiple logs at once.
* **Data Files**: The program requires specific data files to be placed in a central `data/` directory.
    * `cty.dat`: Required for all contests.
    * `ARRLDXmults.dat`: Required for the ARRL DX contest.
    * `SweepstakesSections.dat`: Required for ARRL Sweepstakes.
* **An Environment Variable**: You must tell the program where to find your data files by setting an environment variable named `CONTEST_DATA_DIR`. This variable should point to your `data` directory.

---

## 3. How to Run the Analyzer

The program is run from your command prompt using `main_cli.py`. The basic command structure is:
--- FILE: WPXPrefixLookupAlgorithm.md ---
# WPX Prefix Lookup Algorithm Specification

**Version: 0.28.2-Beta**
**Date: 2025-08-02**

This document describes the algorithm implemented in `cq_wpx_prefix.py` for determining an amateur radio callsign's official CQ WPX Contest prefix.

---
### --- Revision History ---
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's purpose is to implement the specific and complex rules of the CQ WPX Contest to derive the correct prefix from a given callsign. It relies on pre-processed information provided by the `get_cty.py` lookup script to resolve ambiguities.

## 2. Required Inputs

The algorithm requires a row of data containing three key fields:

1.  **`Call`**: The original, raw callsign string.
2.  **`DXCCPfx`**: The resolved DXCC prefix for the location of operation (e.g., "K", "VP2V"). This can be "Unknown".
3.  **`portableid`**: The part of a portable callsign identified by `get_cty.py` as the location designator (e.g., "7", "VP2V"). This is blank for non-portable calls.

## 3. The Lookup Algorithm

The algorithm follows a strict hierarchy. A successful determination at any step concludes the process for that callsign.

### Step 1: Initial Cleanup
The first step is to clean the raw `Call` string to remove common suffixes that are not part of a WPX prefix or a portable designator. This includes stripping suffixes like `/P`, `/M`, `/QRP` and any characters following a hyphen (`-`).

### Step 2: Maritime Mobile Override
The highest-priority rule is for maritime mobile stations. If the raw `Call` string ends in `/MM`, the WPX prefix is immediately determined to be **"Unknown"**.

### Step 3: Portable Call Processing
If the `portableid` field is not blank, the portable prefix rules are applied.

1.  **Prerequisite Check:** If the `DXCCPfx` is "Unknown," the program cannot safely resolve a portable call, and the WPX prefix is **"Unknown"**.
2.  **`call/digit` Rule:** If the `portableid` is a single digit (e.g., "7" from `WN5N/7`), a special transformation is applied: the last digit of the *root callsign's prefix* is replaced with the `portableid` digit. (e.g., `WN5N`'s prefix `WN5` becomes `WN7`).
3.  **Letters-Only Rule:** If the `portableid` contains only letters and no numbers (e.g., "LX" from `LX/KD4D`), a zero (`Ø`) is appended to it to form the prefix (`LXØ`).
4.  **Default Portable Rule:** In all other portable cases (e.g., the `portableid` is a full prefix like "VP2V"), the `portableid` itself becomes the WPX prefix.

### Step 4: Non-Portable Call Processing
If the `portableid` field is blank, the non-portable rules are applied to the cleaned callsign.

1.  **`DXCCPfx` Override Rule:** The program first checks if the callsign starts with the provided `DXCCPfx`. If it does, the `DXCCPfx` is the definitive WPX prefix. This handles special cases like `VP2VMM` (whose `DXCCPfx` is `VP2V`).
2.  **Default Rules:** If the override rule does not apply (either because the call doesn't match or the `DXCCPfx` is "Unknown"), the program uses the following fallback logic:
    * **No-Number Calls:** If the callsign has no numbers (e.g., `RAEM`), the prefix is the first two letters plus a zero (`RAØ`).
    * **Standard Calls:** The prefix is everything in the callsign up to and including the last digit (e.g., `S55A` becomes `S55`).
--- FILE: Working with Gemini.md ---
# Project Workflow Guide

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document outlines the standard operating procedures for developing and documenting the Contest Log Analyzer. Adhering to this workflow ensures consistency, prevents data loss, and allows for efficient collaboration.

---
### --- Revision History ---
- 2025-08-03: Added Section 7, Communication Protocol, to mandate the use of precise and consistent technical terminology.
- 2025-08-02: Clarified rules for versioning, markdown file delivery, and line endings.
- 2025-08-02: Updated workflow with new versioning scheme, file delivery instructions, and debugging/Canvas protocols.
---

## 1. Project File Input

All project source files (`.py`, `.json`) and documentation files (`.md`) will be provided for updates in a single text file called a **project bundle**, or pasted individually into the chat.

The bundle uses a simple text header to separate each file:
`--- FILE: path/to/file.ext ---`

---

## 2. Your Output Format

When you provide updated files back to me, you must follow these rules:

1.  Provide only **one file per response**.
2.  The content inside the block must be the **raw source text**.
3.  To ensure markdown files are delivered correctly without formatting being destroyed, I will use the following specific instruction: **"Provide the entire response as a single, raw markdown code block delivered directly in the chat."**
4.  When following the instruction above, you must enclose the entire file content within a standard fenced code block, using `markdown` as the language specifier (e.g., ```markdown ... ```). This will preserve the raw source text by preventing the chat interface from rendering it.

---

## 3. Versioning and Revision History

1.  All new and modified files will start with version **`0.28.0-Beta`**. When a specific file is modified, its patch number (the third digit) will be incremented. For example, a change to a `0.28.0-Beta` file will result in version `0.28.1-Beta` for that file.
2.  Document versions will be kept in sync with their corresponding code files.
3.  For every file you change, you must update the `Version:` and `Date:` in the file's header comment block and add a new, dated entry to the top of the `--- Revision History ---` block describing the change.

---

## 4. File Verification

My file system uses Windows CRLF (`\r\n`) line endings. This is important for SHA-256 checksum verification. Your output can use standard LF (`\n`) line endings, and you should just be aware of this difference if we need to compare checksums.

---

## 5. Debugging Protocol

When a bug or regression is identified, you will place a greater emphasis on analyzing the specific data and context provided before proposing a solution, rather than relying on general patterns.

---

## 6. Canvas Interface Fallback

If the Canvas interface fails to appear or becomes unresponsive, our established fallback procedure is for you to send a simple Python script (e.g., 'Hello, world!') to the Canvas to restore its functionality.

---

## 7. Communication Protocol

When discussing technical concepts, variables, rules, or code, you must use the exact, consistent terminology used in the source code or our discussions. Do not use conversational synonyms or rephrase technical terms. Precision and consistency are paramount.
--- FILE: documentation_bundle.txt ---
--- FILE: CallsignLookupAlgorithm.md ---
# Callsign Lookup Algorithm Specification

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document describes the algorithm implemented in `get_cty.py` for determining an amateur radio callsign's DXCC entity and other geographical data based on the `CTY.DAT` file.

---
### --- Revision History ---
- 2025-08-03: Replaced the 'Final Fallback' heuristic with a rule to return 'Unknown' for ambiguous calls and inserted the 'ends in a digit' heuristic.
- 2025-08-02: Added `portableid` to the output data structure and a rule to invalidate `digit/call` formats.
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's goal is to replicate the logic of major contest logging programs by implementing a precise, ordered, multi-step algorithm. It takes a raw callsign string as input and returns a data tuple containing the resolved DXCC entity, CQ/ITU Zones, continent, and a new `portableid` field.

## 2. Output Data Structure

The script's output is a `FullCtyInfo` named tuple, which has been modified to include the `portableid` field.

`(DXCCName, DXCCPfx, CQZone, ITUZone, Continent, Lat, Lon, Tzone, WAEName, WAEPfx, portableid)`

The `portableid` field will contain the specific part of a portable callsign that was used to determine the location (e.g., "7", "VP2V") and will be blank for non-portable callsigns.

## 3. The Lookup Algorithm

The script follows a strict order of operations. A successful match at any step concludes the algorithm.

### Step 1: Pre-processing (`_preprocess_callsign`)
The initial step is to clean the raw callsign string to create a standardized base for analysis. This involves stripping common non-prefix suffixes such as `/P`, `/M`, `/QRP`, `/B`, and any characters following a hyphen (`-`).

### Step 2: Exact Match (`_check_exact_match`)
The highest-priority lookup is for an exact match. The `CTY.DAT` file can contain entries prefixed with `=` that map a full, unique callsign to a specific entity. The script checks for these first.

### Step 3: Hardcoded Special Cases (`_check_special_cases`)
The script then checks for hardcoded exceptions that do not follow standard patterns. The primary rules are:
- Any callsign ending in `/MM` (Maritime Mobile) is immediately classified as an "Unknown" entity.
- A specific rule correctly identifies `KG4` callsigns as Guantanamo Bay.

### Step 4: Portable Call Logic (`_handle_portable_call`)
If the cleaned callsign contains a `/`, it is processed by a dedicated handler that uses a series of heuristics to identify the `portableid`. See Section 4 for details.

### Step 5: Longest Prefix Match (`_find_longest_prefix`)
If the call is not resolved by any of the previous steps, this default lookup method is used. It takes the callsign string (e.g., `VP2VMM`) and checks if it is a known prefix. If not, it removes the last character and tries again (`VP2VM`), repeating this process until it finds the longest possible valid prefix (`VP2V`) that exists in the `CTY.DAT` data.

## 4. Portable Call Heuristics

The `_handle_portable_call` method uses the following ordered checks. If a rule is satisfied, a result is returned and the process stops.

1.  **Invalid `digit/call` Format:** The script first checks for the invalid `digit/callsign` format (e.g., `7/KD4D`). If this pattern is found, the call is considered invalid and returns "Unknown".

2.  **Unambiguous Prefix Rule:** The script checks if exactly one side of the `/` is a valid prefix in `cty.dat` while the other is not. If so, the valid side is identified as the `portableid`.

3.  **"Strip the Digit" Heuristic:** If the call is still ambiguous, this tie-breaker temporarily strips a trailing digit from each side. If this makes one side a valid prefix while the other remains invalid, the original, unmodified side that produced the match is chosen as the `portableid`. This is critical for calls like `HC8N/4`.

4.  **US/Canada Heuristic:** This rule handles the `callsign/digit` format for domestic US/Canada calls. If one side appears to have the structure of a US or Canadian callsign and the other is a single digit, the script identifies the **single digit** as the `portableid`.

5.  **"Ends in a Digit" Heuristic:** This is the final tie-breaker. If exactly one side of the `/` ends in a digit while the other does not, the side ending in the digit is identified as the `portableid`. This correctly resolves calls like `WT7/OL5Y`.

6.  **Final Action (No Fallback):** If a call remains ambiguous after all of the above heuristics have been attempted, the script **gives up and returns "Unknown"**. The previous "Final Fallback" logic that attempted to guess the location has been removed to prevent incorrect resolutions.
--- FILE: InstallationGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/InstallationGuide.md

# Installation Guide - Contest Log Analyzer

**Version: 0.26.2-Beta**

This guide provides step-by-step instructions to set up the Contest Log Analyzer on your system. The process involves installing a Python distribution, setting up a dedicated environment, installing the required libraries, and configuring the necessary data files.

---

### Step 1: Prerequisites

Before you begin, you will need to have **Git** installed on your system. Git is a version control system used to download the project's source code. You can download it from [git-scm.com](https://git-scm.com/).

---

### Step 2: Download the Project Files

Open your command prompt or terminal and navigate to the directory where you want to store the project (e.g., your Desktop). Then, run the following command to download the project from its GitHub repository:
--- FILE: ProgrammersGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/ProgrammersGuide.md

# Contest Log Analyzer - Programmer's Guide

**Version: 0.26.2-Beta**

## 1. Introduction

This guide is for developers who want to understand, maintain, or extend the Contest Log Analyzer. It provides an overview of the project's architecture, explains the core data flow, and gives step-by-step instructions for common development tasks like adding new reports, contest definitions, and scoring logic.

---

## 2. Project Architecture & Directory Structure

The project is designed to be highly modular, separating data, processing, and presentation.

* `Contest-Log-Analyzer/` (Project Root)
    * `main_cli.py`: The main command-line interface (CLI) script and the entry point for the application.
    * `contest_tools/`: The core Python application package containing all the processing logic.
        * `cabrillo_parser.py`: Contains low-level functions for reading and parsing the standard Cabrillo log file format.
        * `contest_log.py`: Defines the `ContestLog` class, the central object that holds all data and metadata for a single, fully processed log.
        * `log_manager.py`: Defines the `LogManager` class, which handles loading and managing one or more `ContestLog` instances for comparative analysis.
        * `report_generator.py`: Defines the `ReportGenerator` class, which orchestrates the execution of all reports.
        * `contest_definitions/`: A data-driven package containing JSON files that define the rules, multipliers, and exchange formats for each supported contest.
        * `core_annotations/`: Contains modules for universal data enrichment that applies to most contests, such as country lookup (`get_cty.py`) and Run/S&P classification (`run_s_p.py`).
        * `contest_specific_annotations/`: Contains modules with logic unique to a specific contest, such as the scoring rules for CQ WW or ARRL SS.
        * `reports/`: The "plug-and-play" reporting system. Each Python file in this directory is a self-contained report generator that is automatically discovered by the program.
    * `Logs/` (Recommended User Directory)
        * This directory is the recommended location for storing your raw Cabrillo log files, organized by year and contest.
    * `reports_output/` (Generated Directory)
        * This directory is automatically created by the program to store all generated reports and charts.

---

## 3. Core Concepts & Data Flow

The process follows a clear pipeline:

1.  **Loading**: The `LogManager` is called by `main_cli.py` with paths to raw Cabrillo logs.
2.  **Definition**: The manager reads the `CONTEST:` header from each file to identify the contest (e.g., "CQ-WW-CW"). It then loads the corresponding JSON file (e.g., `cq_ww_cw.json`) into a `ContestDefinition` object.
3.  **Parsing**: The `cabrillo_parser` uses the rules from the `ContestDefinition` object to parse the raw text file into a structured pandas DataFrame and a metadata dictionary.
4.  **Instantiation**: A `ContestLog` object is created to hold the parsed DataFrame and metadata for each log.
5.  **Annotation**: The `ContestLog` object's `apply_annotations()` method is called. This is a crucial step where the raw data is enriched:
    * Core Annotations are applied first (Country/Zone lookup, Run/S&P classification).
    * Contest-Specific Annotations are applied next (QSO point calculation, multiplier identification).
6.  **Reporting**: The final, fully-annotated list of `ContestLog` objects is passed to the `ReportGenerator`, which executes the requested reports based on their defined capabilities (single, pairwise, multi-log).

---

## 4. Extending the Analyzer

### How to Add a New Report

The reporting system is designed to be "plug-and-play." To add a new report, you simply create a new Python file in the `contest_tools/reports/` directory. The system will automatically discover it.

#### The Report Interface

Every report file must contain a class named `Report` that inherits from `ContestReport`. This base class ensures a consistent structure. You must define the following class attributes:

* `report_id`: A unique, machine-readable string for your report (e.g., `band_summary`). This is what the user types on the command line.
* `report_name`: A human-readable name for your report (e.g., "QSOs per Band Summary").
* `report_type`: Must be one of `text`, `plot`, or `chart`. This determines the output subdirectory.
* `supports_single`, `supports_pairwise`, `supports_multi`: Booleans (`True`/`False`) that tell the `ReportGenerator` how to run your report.

You must also implement the `generate(self, output_path: str, **kwargs)` method. This is where your main logic goes. It is responsible for saving its own output file(s) and must return a string confirmation message.

#### Step-by-Step Guide

1.  **Create Your Report File**: In the `contest_tools/reports/` directory, create a new Python file (e.g., `text_my_new_report.py`).
2.  **Use a Template**: Copy the contents of an existing report file (like `text_summary.py`) into your new file to get the correct structure.
3.  **Customize Your Report Class**:
    * Update the class attributes (`report_id`, `report_name`, etc.).
    * Write your analysis logic in the `generate` method.
        * Access the fully processed logs via `self.logs`.
        * Get the pandas DataFrame with `log.get_processed_data()`.
        * Get the header data with `log.get_metadata()`.
        * Safely access optional arguments via `kwargs.get('arg_name', default_value)`.
4.  **Run It!** The system will automatically discover your report.

### How to Add a New Contest Definition

If you want to add support for a contest not currently defined, you only need to create a new JSON file.

1.  **Create JSON File**: In `contest_tools/contest_definitions/`, create a new file (e.g., `arrl_dx_cw.json`).
2.  **Define `contest_name`**: Add the exact name from the Cabrillo `CONTEST:` header (e.g., `"contest_name": "ARRL-DX-CW",`).
3.  **Define Exchange Parsing**: Under `exchange_parsing_rules`, create an entry for the contest name. Provide a regex to capture the exchange fields and a list of `groups` to name them.
4.  **Define Multipliers**: Add a `multiplier_rules` list to define the multipliers for the contest (e.g., states, countries). Specify the source of the multiplier data.
5.  **(Optional) Add Scoring**: If the contest requires custom scoring, see the next section.

### How to Add New Contest-Specific Scoring

The system can dynamically load scoring logic for any contest.

1.  **Create Scoring File**: In `contest_tools/contest_specific_annotations/`, create a new Python file whose name matches the contest's JSON file (e.g., `arrl_dx_scoring.py`).
2.  **Implement `calculate_points`**: The file must contain a function with the signature `calculate_points(df: pd.DataFrame, my_call_info: Dict[str, Any]) -> pd.Series`.
3.  **Write Logic**: Inside this function, write the logic to calculate the point value for each QSO in the input DataFrame (`df`). The `my_call_info` dictionary provides the operator's own location data, which is often needed for scoring.
4.  **Automatic Discovery**: The `ContestLog` class will automatically find and execute this function during the annotation process based on the contest name.
--- FILE: README.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/README.md

# Contest Log Analyzer

**Version: 0.26.2-Beta**

A Python-based tool for in-depth analysis and comparison of amateur radio contest logs. This application processes standard Cabrillo files to generate detailed reports, charts, and visualizations, providing deep insights into operator strategy and performance.

## Key Features

* **Data-Driven Architecture**: Uses simple JSON files to define the rules, scoring, and exchange formats for each contest, making the tool highly extensible.
* **Advanced Log Processing**: Ingests standard Cabrillo files and enriches the data with universal annotations (DXCC, WAE, Continent, Zones) and contest-specific data.
* **Run/S&P/Unknown Classification**: A sophisticated, multi-pass heuristic analyzes QSO patterns to classify each contact as "Run," "Search & Pounce," or "Unknown," providing a clear picture of operating strategy.
* **Contest-Specific Scoring**: A modular system calculates QSO points based on the official rules for supported contests (ARRL-DX, ARRL-SS, CQ-WPX, CQ-WW).
* **Dynamic Reporting Engine**: A flexible, "plug-and-play" system for generating a wide variety of text, plot, and chart-based reports.
* **Comparative Analysis**: Most reports are designed for pairwise or multi-log comparison, allowing you to analyze your performance against others.

## Installation

This project uses `conda` for environment and package management.

#### 1. Clone the Repository

```
git clone [https://github.com/kd4d/Contest-Log-Analyzer.git](https://github.com/kd4d/Contest-Log-Analyzer.git) "Contest-Log-Analyzer"
cd "Contest-Log-Analyzer"
```

#### 2. Create and Activate Conda Environment

It is recommended to use Miniforge and create a dedicated environment.

```
conda create --name contest-analyzer python=3.11 -y
conda activate contest-analyzer
```

#### 3. Install Dependencies

Update the base packages and then install the required libraries.

```
conda update --all -y
conda install pandas matplotlib seaborn -y
```

#### 4. Set Up Environment Variable

The program requires the `CONTEST_DATA_DIR` environment variable to be set to the location of your data directory.

* **Windows (Temporary):**
    ```
    set CONTEST_DATA_DIR="C:\path\to\your\Contest-Log-Analyzer\data"
    ```
* **macOS/Linux (Temporary):**
    ```
    export CONTEST_DATA_DIR="/path/to/your/Contest-Log-Analyzer/data"
    ```

#### 5. Download Data Files

Place the necessary data files in a central `data/` directory.

* **Required for all contests:** `cty.dat` (from [country-files.com](http://www.country-files.com/cty/cty.dat))
* **Required for ARRL DX:** `ARRLDXmults.dat`
* **Required for ARRL SS:** `SweepstakesSections.dat`

## Usage

The analyzer is run from the command line using `main_cli.py`.

#### **Basic Syntax**

```
python main_cli.py --report <ReportID|all> <LogFile1> [<LogFile2>...] [options]
```

#### **Examples**

* **Generate all available reports for two logs:**
    ```
    python main_cli.py --report all Logs/2024/cq-ww-cw/k3lr.log Logs/2024/cq-ww-cw/kc1xx.log
    ```
* **Generate a specific report (Score Summary) for a single log:**
    ```
    python main_cli.py --report score_report Logs/2024/cq-ww-cw/k3lr.log
    ```
* **Generate a Missed Multipliers report for CQ WW Zones:**
    ```
    python main_cli.py --report missed_multipliers --mult-name Zones Logs/2024/cq-ww-cw/k3lr.log Logs/2024/cq-ww-cw/kc1xx.log
    ```

## Available Reports

All generated files are saved to a structured directory under `reports_output/YYYY/CONTEST_NAME/`.

#### **Text Reports (`text/`)**

* `summary`: High-level overview of QSO counts (Run, S&P, Unknown).
* `score_report`: Comprehensive score breakdown by band for a single log.
* `rate_sheet`: Detailed hourly QSO rates per band for a single log.
* `rate_sheet_comparison`: Side-by-side hourly rate comparison for multiple logs.
* `qso_comparison`: Detailed pairwise breakdown of Total, Unique, and Common QSOs.
* `missed_multipliers`: Comparative report showing multipliers missed by each station.
* `multiplier_summary`: Detailed breakdown of QSOs per multiplier.
* `multipliers_by_hour`: Shows new multipliers worked each hour of the contest.
* `continent_summary`: Total QSOs per continent for a single log.
* `comparative_continent_summary`: Side-by-side comparison of QSOs per continent.
* `continent_breakdown`: Detailed QSOs per continent broken down by Run/S&P status.

#### **Plots (`plots/`)**

* `qso_rate_plots`: Cumulative QSO rate line graphs.
* `point_rate_plots`: Cumulative point rate line graphs.
* `cumulative_difference_plots`: Plot showing the running QSO or Point difference between two logs.

#### **Charts (`charts/`)**

* `qso_breakdown_chart`: Stacked bar chart comparing unique QSO counts for two logs.
* `chart_point_contribution`: Side-by-side pie charts comparing point sources.
* `chart_point_contribution_single`: Per-band pie charts showing point sources for one log.

## License

This project is licensed under the **Mozilla Public License, v. 2.0**.

--- FILE: ReportInterpretationGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/ReportInterpretationGuide.md

# Report Interpretation Guide

**Version: 0.26.3-Beta**

## 1. Introduction

This guide explains how to read and interpret the various reports generated by the Contest Log Analyzer. The goal is not just to understand the numbers, but to turn them into actionable insights that can help you improve your scores in future contests.

All examples in this guide are from the **2024 CQ WW DX CW Contest**, comparing the logs from K1LZ, K3LR, and KC1XX.

---

## 2. Text Reports

Text reports provide detailed, granular data in a plain-text format. They are best for deep-dive analysis and finding specific details.

### Score Report (`score_report`)

This is a comprehensive, single-log summary of your final score, broken down by band. It provides the most important top-level metrics for your operation.

#### Example Output

<!-- Text from score_report_K3LR.txt will be inserted here. -->

#### How to Interpret This Report

* **AVG (Average Points per QSO)**: This is a crucial metric. An AVG close to 3.0 indicates a strong focus on high-value inter-continental QSOs. An AVG closer to 1.0 or 2.0 suggests more contacts within your own continent.
* **Multiplier Totals**: For contests like CQ WW, multipliers are counted on each band. The `TOTAL` row shows the *sum* of multipliers from each band, not the number of unique multipliers.

---

### QSO Summary (`summary`)

This report provides a high-level comparative overview of the total QSO counts and operating styles for all analyzed logs.

#### Example Output

<!-- Text from summary_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

This report is the quickest way to understand fundamental differences in operating strategy. In this example, all three stations have a very similar **Run-to-S&P ratio** (roughly 2:1), indicating a shared, effective strategy focused on calling CQ ("Running") as much as possible.

---

### Rate Sheet (`rate_sheet` & `rate_sheet_comparison`)

The `rate_sheet` provides an hour-by-hour breakdown of QSO rates for a single log, while the `rate_sheet_comparison` places this data side-by-side for multiple logs.

#### Example Output (Comparison)

<!-- Text from rate_sheet_comparison_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

Use this report to analyze strategic differences in band selection and pacing. At `0000Z`, K1LZ focused heavily on 40M, while K3LR was already making significant numbers on 10M, indicating a different approach to chasing band openings.

---

### QSO Comparison Summary (`qso_comparison`)

This powerful pairwise report breaks down how two logs compare on each band, focusing on which QSOs were **unique** to each station and which were **common** to both.

#### Example Output (10 Meter Band)

<!-- Text from qso_comparison_K3LR_vs_K1LZ.txt will be inserted here. -->

#### How to Interpret This Report

The **Unique** columns reveal the real strategy. K3LR's 878 unique QSOs came primarily from "Running" (745 QSOs). This means K3LR's high rate on 10M was effective at attracting stations that K1LZ never logged. A high number of "Unique S&P" QSOs would indicate that one operator was more effective at searching for and finding rare stations that the other missed.

---

### Missed Multipliers (`missed_multipliers`)

This report is essential for identifying the most costly unworked stations. It lists every multiplier that was worked by at least one person in the group but missed by at least one other, showing who worked it and how.

#### Example Output (10 Meter Band Snippet)

<!-- Text from missed_multipliers_countries_K1LZ_vs_K3LR_vs_KC1XX.txt will be inserted here. -->

#### How to Interpret This Report

The text `(Run)`, `(S&P)`, or `(Both)` shows how the station logged that multiplier. This is critical information. If your competitor worked a rare multiplier via `(S&P)`, it means they found it by tuning the band. If they got it via `(Run)`, it means that multiplier *called them*.

---

## 3. Plots and Charts

Plots and charts provide a high-level, visual summary of performance.

### QSO Rate Plot (`qso_rate_plots`)

This plot shows the cumulative QSO total over the course of the contest. It's the best way to visualize the overall "horse race."

<!-- There will be an image/file inserted here: qso_rate_all_plot.png -->

#### How to Interpret This Plot

* **Slope of the Line**: A steeper slope indicates a higher QSO rate. You can see periods where one station pulls ahead or another catches up.
* **Plateaus**: Flat sections of the graph indicate off-times or periods of very low activity.
* **Inset Table**: The table provides a convenient summary of the final QSO totals and the Run/S&P/Unknown breakdown for each station.

---

### Cumulative Difference Plot (`cumulative_difference_plots`)

This unique plot is one of the most powerful analysis tools in the package. It visualizes the *difference* between two logs over time, broken down by operating style.

<!-- There will be an image/file inserted here: diff_qsos_all_K3LR_vs_KC1XX.png -->

#### How to Interpret This Plot

This plot shows **K3LR minus KC1XX**.

* **Top Panel (Overall Diff)**: This shows the total QSO difference. When the line is above zero, K3LR is ahead. When it drops below zero, KC1XX has taken the lead.
* **Middle Panel (Run Diff)**: This isolates the difference in **Run** QSOs. K3LR's initial lead was built on a superior run rate, but the downward trend shows KC1XX consistently out-ran K3LR after the first few hours.
* **Bottom Panel (S&P+Unk Diff)**: This isolates the difference in **S&P** QSOs. The upward trend shows K3LR was significantly more effective at Search & Pounce.
* **Strategic Insight**: This plot tells a clear story. K3LR's S&P strategy was superior, but it was not enough to overcome the deficit from KC1XX's more powerful run rate in the second half of the contest.

---

### QSO Breakdown Chart (`qso_breakdown_chart`)

This chart provides a visual companion to the `qso_comparison` text report, breaking down the unique and common QSOs on each band.

<!-- There will be an image/file inserted here: qso_breakdown_chart_K3LR_vs_K1LZ.png -->

#### How to Interpret This Chart

This chart is unique to this analyzer and reveals where each station gained its advantage.

* **Gray Bar (Common)**: This represents the base of QSOs that both stations worked.
* **Colored Bars (Unique)**: The stacked, colored bars on either side show the QSOs that were unique to that station, broken down by Run, S&P, and Unknown.
* **Strategic Insight**: On 20 meters, the high "Common" bar shows both stations worked a similar pool of stations. However, both added a significant number of **Unique Run** QSOs (the large red segments), reinforcing that they were successfully running different sets of stations.

---

### Point Contribution Breakdown (`chart_point_contribution`)

This chart shows where your points came from. For CQ WW, points are awarded based on the continent of the station worked.

<!-- There will be an image/file inserted here: chart_point_contribution_all_bands_K1LZ_vs_K3LR_vs_KC1XX.png -->

#### How to Interpret This Chart

* **Pie Chart**: The slices show the proportion of total points that came from each point value. In CQ WW, 3-point inter-continental QSOs are the most valuable.
* **Table**: The table below the chart provides the exact counts for each point category and the final average points per QSO.
--- FILE: RunS&PAlgorithm.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/RunS&PAlgorithm.md

# Run/S&P/Unknown Classification Algorithm

**Version: 0.26.2-Beta**

The purpose of this algorithm is to analyze a contest log and infer the operator's activity for each contact, classifying it as one of three types: **Run**, **Search & Pounce (S&P)**, or **Unknown**.

The analysis is performed independently for each operating "stream"—a unique combination of a band and mode (e.g., 20M CW is one stream, 40M SSB is another). The algorithm uses a two-pass approach.

---

### Pass 1: Initial Classification (Run vs. S&P)

The first pass uses a "sticky run" state machine to make an initial classification.

1.  **Identifying a Run**: A "run" is defined as a period of high-rate activity on a single frequency. The algorithm identifies the start of a run when it detects a minimum number of QSOs (typically 3) occurring on the same frequency within a short time window (e.g., 10 minutes).
2.  **The "Sticky" State**: Once a run is identified, the algorithm enters a "run state." It assumes the operator is still running and will continue to classify all subsequent QSOs on that frequency as **Run**.
3.  **Breaking a Run**: The run state is maintained until one of two conditions is met:
    * **Time-Out**: A significant amount of time (e.g., 2 minutes) passes without a QSO on the run frequency.
    * **Frequency Change**: The operator makes several consecutive QSOs on other frequencies, indicating they have moved to search for new contacts.
4.  **S&P Classification**: Any QSO that is not part of an identified run is initially classified as **S&P**.

---

### Pass 2: Reclassification of Low-Rate QSOs

The second pass refines the results by identifying periods of very low activity where the operator's intent is ambiguous.

1.  **Reviewing S&P QSOs**: The algorithm re-examines only the QSOs that were classified as **S&P** in the first pass.
2.  **Checking the Rate**: For each S&P QSO, it looks at the number of other contacts made in a time window both before and after it (e.g., 15 minutes).
3.  **Reclassifying to Unknown**: If the QSO rate in the surrounding period is below a certain threshold, the activity is considered too low to be definitively classified. In this case, the QSO's status is changed from S&P to **Unknown**.

The final output is a log where every contact is annotated with its inferred operating style: **Run**, **S&P**, or **Unknown**.
--- FILE: UsersGuide.md ---
# Contest-Log-Analyzer/Docs/Version 0.26.0-Beta/UsersGuide.md

# Contest Log Analyzer - User Guide

**Version: 0.26.2-Beta**

## 1. Introduction: What is the Contest Log Analyzer?

The Contest Log Analyzer is a powerful command-line tool designed for amateur radio contesters who want to perform deep, data-driven analysis of their operating performance. It goes beyond the simple score summary provided by most logging software, allowing you to:

* Process raw Cabrillo log files into a clean, standardized format.
* Automatically classify every QSO as "Run," "Search & Pounce," or "Unknown" to analyze your operating strategy.
* Generate detailed reports and charts that compare your log against one or more others.
* Analyze performance on a band-by-band basis to identify strengths and weaknesses.
* Calculate contest-specific QSO points for supported contests.

The ultimate goal of this program is to help you understand your contest operation in minute detail, identify missed opportunities, and improve your strategy for the next event.

---

## 2. What You Need to Get Started

Before running the analyzer, you will need a few files:

* **Your Cabrillo Log File(s)**: These are the standard log files generated by your contest logging software (e.g., `k3lr.log`, `kc1xx.log`). You can analyze a single log or compare multiple logs at once.
* **Data Files**: The program requires specific data files to be placed in a central `data/` directory.
    * `cty.dat`: Required for all contests.
    * `ARRLDXmults.dat`: Required for the ARRL DX contest.
    * `SweepstakesSections.dat`: Required for ARRL Sweepstakes.
* **An Environment Variable**: You must tell the program where to find your data files by setting an environment variable named `CONTEST_DATA_DIR`. This variable should point to your `data` directory.

---

## 3. How to Run the Analyzer

The program is run from your command prompt using `main_cli.py`. The basic command structure is:
--- FILE: WPXPrefixLookupAlgorithm.md ---
# WPX Prefix Lookup Algorithm Specification

**Version: 0.28.2-Beta**
**Date: 2025-08-02**

This document describes the algorithm implemented in `cq_wpx_prefix.py` for determining an amateur radio callsign's official CQ WPX Contest prefix.

---
### --- Revision History ---
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's purpose is to implement the specific and complex rules of the CQ WPX Contest to derive the correct prefix from a given callsign. It relies on pre-processed information provided by the `get_cty.py` lookup script to resolve ambiguities.

## 2. Required Inputs

The algorithm requires a row of data containing three key fields:

1.  **`Call`**: The original, raw callsign string.
2.  **`DXCCPfx`**: The resolved DXCC prefix for the location of operation (e.g., "K", "VP2V"). This can be "Unknown".
3.  **`portableid`**: The part of a portable callsign identified by `get_cty.py` as the location designator (e.g., "7", "VP2V"). This is blank for non-portable calls.

## 3. The Lookup Algorithm

The algorithm follows a strict hierarchy. A successful determination at any step concludes the process for that callsign.

### Step 1: Initial Cleanup
The first step is to clean the raw `Call` string to remove common suffixes that are not part of a WPX prefix or a portable designator. This includes stripping suffixes like `/P`, `/M`, `/QRP` and any characters following a hyphen (`-`).

### Step 2: Maritime Mobile Override
The highest-priority rule is for maritime mobile stations. If the raw `Call` string ends in `/MM`, the WPX prefix is immediately determined to be **"Unknown"**.

### Step 3: Portable Call Processing
If the `portableid` field is not blank, the portable prefix rules are applied.

1.  **Prerequisite Check:** If the `DXCCPfx` is "Unknown," the program cannot safely resolve a portable call, and the WPX prefix is **"Unknown"**.
2.  **`call/digit` Rule:** If the `portableid` is a single digit (e.g., "7" from `WN5N/7`), a special transformation is applied: the last digit of the *root callsign's prefix* is replaced with the `portableid` digit. (e.g., `WN5N`'s prefix `WN5` becomes `WN7`).
3.  **Letters-Only Rule:** If the `portableid` contains only letters and no numbers (e.g., "LX" from `LX/KD4D`), a zero (`Ø`) is appended to it to form the prefix (`LXØ`).
4.  **Default Portable Rule:** In all other portable cases (e.g., the `portableid` is a full prefix like "VP2V"), the `portableid` itself becomes the WPX prefix.

### Step 4: Non-Portable Call Processing
If the `portableid` field is blank, the non-portable rules are applied to the cleaned callsign.

1.  **`DXCCPfx` Override Rule:** The program first checks if the callsign starts with the provided `DXCCPfx`. If it does, the `DXCCPfx` is the definitive WPX prefix. This handles special cases like `VP2VMM` (whose `DXCCPfx` is `VP2V`).
2.  **Default Rules:** If the override rule does not apply (either because the call doesn't match or the `DXCCPfx` is "Unknown"), the program uses the following fallback logic:
    * **No-Number Calls:** If the callsign has no numbers (e.g., `RAEM`), the prefix is the first two letters plus a zero (`RAØ`).
    * **Standard Calls:** The prefix is everything in the callsign up to and including the last digit (e.g., `S55A` becomes `S55`).
--- FILE: Working with Gemini.md ---
# Project Workflow Guide

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document outlines the standard operating procedures for developing and documenting the Contest Log Analyzer. Adhering to this workflow ensures consistency, prevents data loss, and allows for efficient collaboration.

---
### --- Revision History ---
- 2025-08-03: Added Section 7, Communication Protocol, to mandate the use of precise and consistent technical terminology.
- 2025-08-02: Clarified rules for versioning, markdown file delivery, and line endings.
- 2025-08-02: Updated workflow with new versioning scheme, file delivery instructions, and debugging/Canvas protocols.
---

## 1. Project File Input

All project source files (`.py`, `.json`) and documentation files (`.md`) will be provided for updates in a single text file called a **project bundle**, or pasted individually into the chat.

The bundle uses a simple text header to separate each file:
`--- FILE: path/to/file.ext ---`

---

## 2. Your Output Format

When you provide updated files back to me, you must follow these rules:

1.  Provide only **one file per response**.
2.  The content inside the block must be the **raw source text**.
3.  To ensure markdown files are delivered correctly without formatting being destroyed, I will use the following specific instruction: **"Provide the entire response as a single, raw markdown code block delivered directly in the chat."**
4.  When following the instruction above, you must enclose the entire file content within a standard fenced code block, using `markdown` as the language specifier (e.g., ```markdown ... ```). This will preserve the raw source text by preventing the chat interface from rendering it.

---

## 3. Versioning and Revision History

1.  All new and modified files will start with version **`0.28.0-Beta`**. When a specific file is modified, its patch number (the third digit) will be incremented. For example, a change to a `0.28.0-Beta` file will result in version `0.28.1-Beta` for that file.
2.  Document versions will be kept in sync with their corresponding code files.
3.  For every file you change, you must update the `Version:` and `Date:` in the file's header comment block and add a new, dated entry to the top of the `--- Revision History ---` block describing the change.

---

## 4. File Verification

My file system uses Windows CRLF (`\r\n`) line endings. This is important for SHA-256 checksum verification. Your output can use standard LF (`\n`) line endings, and you should just be aware of this difference if we need to compare checksums.

---

## 5. Debugging Protocol

When a bug or regression is identified, you will place a greater emphasis on analyzing the specific data and context provided before proposing a solution, rather than relying on general patterns.

---

## 6. Canvas Interface Fallback

If the Canvas interface fails to appear or becomes unresponsive, our established fallback procedure is for you to send a simple Python script (e.g., 'Hello, world!') to the Canvas to restore its functionality.

---

## 7. Communication Protocol

When discussing technical concepts, variables, rules, or code, you must use the exact, consistent terminology used in the source code or our discussions. Do not use conversational synonyms or rephrase technical terms. Precision and consistency are paramount.
--- FILE: documentation_bundle1.txt ---
--- FILE: CallsignLookupAlgorithm.md ---
# Callsign Lookup Algorithm Specification

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document describes the algorithm implemented in `get_cty.py` for determining an amateur radio callsign's DXCC entity and other geographical data based on the `CTY.DAT` file.

---
### --- Revision History ---
- 2025-08-03: Replaced the 'Final Fallback' heuristic with a rule to return 'Unknown' for ambiguous calls and inserted the 'ends in a digit' heuristic.
- 2025-08-02: Added `portableid` to the output data structure and a rule to invalidate `digit/call` formats.
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's goal is to replicate the logic of major contest logging programs by implementing a precise, ordered, multi-step algorithm. It takes a raw callsign string as input and returns a data tuple containing the resolved DXCC entity, CQ/ITU Zones, continent, and a new `portableid` field.

## 2. Output Data Structure

The script's output is a `FullCtyInfo` named tuple, which has been modified to include the `portableid` field.

`(DXCCName, DXCCPfx, CQZone, ITUZone, Continent, Lat, Lon, Tzone, WAEName, WAEPfx, portableid)`

The `portableid` field will contain the specific part of a portable callsign that was used to determine the location (e.g., "7", "VP2V") and will be blank for non-portable callsigns.

## 3. The Lookup Algorithm

The script follows a strict order of operations. A successful match at any step concludes the algorithm.

### Step 1: Pre-processing (`_preprocess_callsign`)
The initial step is to clean the raw callsign string to create a standardized base for analysis. This involves stripping common non-prefix suffixes such as `/P`, `/M`, `/QRP`, `/B`, and any characters following a hyphen (`-`).

### Step 2: Exact Match (`_check_exact_match`)
The highest-priority lookup is for an exact match. The `CTY.DAT` file can contain entries prefixed with `=` that map a full, unique callsign to a specific entity. The script checks for these first.

### Step 3: Hardcoded Special Cases (`_check_special_cases`)
The script then checks for hardcoded exceptions that do not follow standard patterns. The primary rules are:
- Any callsign ending in `/MM` (Maritime Mobile) is immediately classified as an "Unknown" entity.
- A specific rule correctly identifies `KG4` callsigns as Guantanamo Bay.

### Step 4: Portable Call Logic (`_handle_portable_call`)
If the cleaned callsign contains a `/`, it is processed by a dedicated handler that uses a series of heuristics to identify the `portableid`. See Section 4 for details.

### Step 5: Longest Prefix Match (`_find_longest_prefix`)
If the call is not resolved by any of the previous steps, this default lookup method is used. It takes the callsign string (e.g., `VP2VMM`) and checks if it is a known prefix. If not, it removes the last character and tries again (`VP2VM`), repeating this process until it finds the longest possible valid prefix (`VP2V`) that exists in the `CTY.DAT` data.

## 4. Portable Call Heuristics

The `_handle_portable_call` method uses the following ordered checks. If a rule is satisfied, a result is returned and the process stops.

1.  **Invalid `digit/call` Format:** The script first checks for the invalid `digit/callsign` format (e.g., `7/KD4D`). If this pattern is found, the call is considered invalid and returns "Unknown".

2.  **Unambiguous Prefix Rule:** The script checks if exactly one side of the `/` is a valid prefix in `cty.dat` while the other is not. If so, the valid side is identified as the `portableid`.

3.  **"Strip the Digit" Heuristic:** If the call is still ambiguous, this tie-breaker temporarily strips a trailing digit from each side. If this makes one side a valid prefix while the other remains invalid, the original, unmodified side that produced the match is chosen as the `portableid`. This is critical for calls like `HC8N/4`.

4.  **US/Canada Heuristic:** This rule handles the `callsign/digit` format for domestic US/Canada calls. If one side appears to have the structure of a US or Canadian callsign and the other is a single digit, the script identifies the **single digit** as the `portableid`.

5.  **"Ends in a Digit" Heuristic:** This is the final tie-breaker. If exactly one side of the `/` ends in a digit while the other does not, the side ending in the digit is identified as the `portableid`. This correctly resolves calls like `WT7/OL5Y`.

6.  **Final Action (No Fallback):** If a call remains ambiguous after all of the above heuristics have been attempted, the script **gives up and returns "Unknown"**. The previous "Final Fallback" logic that attempted to guess the location has been removed to prevent incorrect resolutions.
--- FILE: WPXPrefixLookupAlgorithm.md ---
# WPX Prefix Lookup Algorithm Specification

**Version: 0.28.2-Beta**
**Date: 2025-08-02**

This document describes the algorithm implemented in `cq_wpx_prefix.py` for determining an amateur radio callsign's official CQ WPX Contest prefix.

---
### --- Revision History ---
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's purpose is to implement the specific and complex rules of the CQ WPX Contest to derive the correct prefix from a given callsign. It relies on pre-processed information provided by the `get_cty.py` lookup script to resolve ambiguities.

## 2. Required Inputs

The algorithm requires a row of data containing three key fields:

1.  **`Call`**: The original, raw callsign string.
2.  **`DXCCPfx`**: The resolved DXCC prefix for the location of operation (e.g., "K", "VP2V"). This can be "Unknown".
3.  **`portableid`**: The part of a portable callsign identified by `get_cty.py` as the location designator (e.g., "7", "VP2V"). This is blank for non-portable calls.

## 3. The Lookup Algorithm

The algorithm follows a strict hierarchy. A successful determination at any step concludes the process for that callsign.

### Step 1: Initial Cleanup
The first step is to clean the raw `Call` string to remove common suffixes that are not part of a WPX prefix or a portable designator. This includes stripping suffixes like `/P`, `/M`, `/QRP` and any characters following a hyphen (`-`).

### Step 2: Maritime Mobile Override
The highest-priority rule is for maritime mobile stations. If the raw `Call` string ends in `/MM`, the WPX prefix is immediately determined to be **"Unknown"**.

### Step 3: Portable Call Processing
If the `portableid` field is not blank, the portable prefix rules are applied.

1.  **Prerequisite Check:** If the `DXCCPfx` is "Unknown," the program cannot safely resolve a portable call, and the WPX prefix is **"Unknown"**.
2.  **`call/digit` Rule:** If the `portableid` is a single digit (e.g., "7" from `WN5N/7`), a special transformation is applied: the last digit of the *root callsign's prefix* is replaced with the `portableid` digit. (e.g., `WN5N`'s prefix `WN5` becomes `WN7`).
3.  **Letters-Only Rule:** If the `portableid` contains only letters and no numbers (e.g., "LX" from `LX/KD4D`), a zero (`Ø`) is appended to it to form the prefix (`LXØ`).
4.  **Default Portable Rule:** In all other portable cases (e.g., the `portableid` is a full prefix like "VP2V"), the `portableid` itself becomes the WPX prefix.

### Step 4: Non-Portable Call Processing
If the `portableid` field is blank, the non-portable rules are applied to the cleaned callsign.

1.  **`DXCCPfx` Override Rule:** The program first checks if the callsign starts with the provided `DXCCPfx`. If it does, the `DXCCPfx` is the definitive WPX prefix. This handles special cases like `VP2VMM` (whose `DXCCPfx` is `VP2V`).
2.  **Default Rules:** If the override rule does not apply (either because the call doesn't match or the `DXCCPfx` is "Unknown"), the program uses the following fallback logic:
    * **No-Number Calls:** If the callsign has no numbers (e.g., `RAEM`), the prefix is the first two letters plus a zero (`RAØ`).
    * **Standard Calls:** The prefix is everything in the callsign up to and including the last digit (e.g., `S55A` becomes `S55`).
--- FILE: Working with Gemini.md ---
# Project Workflow Guide

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document outlines the standard operating procedures for developing and documenting the Contest Log Analyzer. Adhering to this workflow ensures consistency, prevents data loss, and allows for efficient collaboration.

---
### --- Revision History ---
- 2025-08-03: Added Section 7, Communication Protocol, to mandate the use of precise and consistent technical terminology.
- 2025-08-02: Clarified rules for versioning, markdown file delivery, and line endings.
- 2025-08-02: Updated workflow with new versioning scheme, file delivery instructions, and debugging/Canvas protocols.
---

## 1. Project File Input

All project source files (`.py`, `.json`) and documentation files (`.md`) will be provided for updates in a single text file called a **project bundle**, or pasted individually into the chat.

The bundle uses a simple text header to separate each file:
`--- FILE: path/to/file.ext ---`

---

## 2. Your Output Format

When you provide updated files back to me, you must follow these rules:

1.  Provide only **one file per response**.
2.  The content inside the block must be the **raw source text**.
3.  To ensure markdown files are delivered correctly without formatting being destroyed, I will use the following specific instruction: **"Provide the entire response as a single, raw markdown code block delivered directly in the chat."**
4.  When following the instruction above, you must enclose the entire file content within a standard fenced code block, using `markdown` as the language specifier (e.g., ```markdown ... ```). This will preserve the raw source text by preventing the chat interface from rendering it.

---

## 3. Versioning and Revision History

1.  All new and modified files will start with version **`0.28.0-Beta`**. When a specific file is modified, its patch number (the third digit) will be incremented. For example, a change to a `0.28.0-Beta` file will result in version `0.28.1-Beta` for that file.
2.  Document versions will be kept in sync with their corresponding code files.
3.  For every file you change, you must update the `Version:` and `Date:` in the file's header comment block and add a new, dated entry to the top of the `--- Revision History ---` block describing the change.

---

## 4. File Verification

My file system uses Windows CRLF (`\r\n`) line endings. This is important for SHA-256 checksum verification. Your output can use standard LF (`\n`) line endings, and you should just be aware of this difference if we need to compare checksums.

---

## 5. Debugging Protocol

When a bug or regression is identified, you will place a greater emphasis on analyzing the specific data and context provided before proposing a solution, rather than relying on general patterns.

---

## 6. Canvas Interface Fallback

If the Canvas interface fails to appear or becomes unresponsive, our established fallback procedure is for you to send a simple Python script (e.g., 'Hello, world!') to the Canvas to restore its functionality.

---

## 7. Communication Protocol

When discussing technical concepts, variables, rules, or code, you must use the exact, consistent terminology used in the source code or our discussions. Do not use conversational synonyms or rephrase technical terms. Precision and consistency are paramount.
--- FILE: documentation_bundle.txt ---
--- FILE: CallsignLookupAlgorithm.md ---
# Callsign Lookup Algorithm Specification

**Version: 0.28.8-Beta**
**Date: 2025-08-03**

This document describes the algorithm implemented in `get_cty.py` for determining an amateur radio callsign's DXCC entity and other geographical data based on the `CTY.DAT` file.

---
### --- Revision History ---
- 2025-08-03: Replaced the 'Final Fallback' heuristic with a rule to return 'Unknown' for ambiguous calls and inserted the 'ends in a digit' heuristic.
- 2025-08-02: Added `portableid` to the output data structure and a rule to invalidate `digit/call` formats.
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's goal is to replicate the logic of major contest logging programs by implementing a precise, ordered, multi-step algorithm. It takes a raw callsign string as input and returns a data tuple containing the resolved DXCC entity, CQ/ITU Zones, continent, and a new `portableid` field.

## 2. Output Data Structure

The script's output is a `FullCtyInfo` named tuple, which has been modified to include the `portableid` field.

`(DXCCName, DXCCPfx, CQZone, ITUZone, Continent, Lat, Lon, Tzone, WAEName, WAEPfx, portableid)`

The `portableid` field will contain the specific part of a portable callsign that was used to determine the location (e.g., "7", "VP2V") and will be blank for non-portable callsigns.

## 3. The Lookup Algorithm

The script follows a strict order of operations. A successful match at any step concludes the algorithm.

### Step 1: Pre-processing (`_preprocess_callsign`)
The initial step is to clean the raw callsign string to create a standardized base for analysis. This involves stripping common non-prefix suffixes such as `/P`, `/M`, `/QRP`, `/B`, and any characters following a hyphen (`-`).

### Step 2: Exact Match (`_check_exact_match`)
The highest-priority lookup is for an exact match. The `CTY.DAT` file can contain entries prefixed with `=` that map a full, unique callsign to a specific entity. The script checks for these first.

### Step 3: Hardcoded Special Cases (`_check_special_cases`)
The script then checks for hardcoded exceptions that do not follow standard patterns. The primary rules are:
- Any callsign ending in `/MM` (Maritime Mobile) is immediately classified as an "Unknown" entity.
- A specific rule correctly identifies `KG4` callsigns as Guantanamo Bay.

### Step 4: Portable Call Logic (`_handle_portable_call`)
If the cleaned callsign contains a `/`, it is processed by a dedicated handler that uses a series of heuristics to identify the `portableid`. See Section 4 for details.

### Step 5: Longest Prefix Match (`_find_longest_prefix`)
If the call is not resolved by any of the previous steps, this default lookup method is used. It takes the callsign string (e.g., `VP2VMM`) and checks if it is a known prefix. If not, it removes the last character and tries again (`VP2VM`), repeating this process until it finds the longest possible valid prefix (`VP2V`) that exists in the `CTY.DAT` data.

## 4. Portable Call Heuristics

The `_handle_portable_call` method uses the following ordered checks. If a rule is satisfied, a result is returned and the process stops.

1.  **Invalid `digit/call` Format:** The script first checks for the invalid `digit/callsign` format (e.g., `7/KD4D`). If this pattern is found, the call is considered invalid and returns "Unknown".

2.  **Unambiguous Prefix Rule:** The script checks if exactly one side of the `/` is a valid prefix in `cty.dat` while the other is not. If so, the valid side is identified as the `portableid`.

3.  **"Strip the Digit" Heuristic:** If the call is still ambiguous, this tie-breaker temporarily strips a trailing digit from each side. If this makes one side a valid prefix while the other remains invalid, the original, unmodified side that produced the match is chosen as the `portableid`. This is critical for calls like `HC8N/4`.

4.  **US/Canada Heuristic:** This rule handles the `callsign/digit` format for domestic US/Canada calls. If one side appears to have the structure of a US or Canadian callsign and the other is a single digit, the script identifies the **single digit** as the `portableid`.

5.  **"Ends in a Digit" Heuristic:** This is the final tie-breaker. If exactly one side of the `/` ends in a digit while the other does not, the side ending in the digit is identified as the `portableid`. This correctly resolves calls like `WT7/OL5Y`.

6.  **Final Action (No Fallback):** If a call remains ambiguous after all of the above heuristics have been attempted, the script **gives up and returns "Unknown"**. The previous "Final Fallback" logic that attempted to guess the location has been removed to prevent incorrect resolutions.
--- FILE: WPXPrefixLookupAlgorithm.md ---
# WPX Prefix Lookup Algorithm Specification

**Version: 0.28.2-Beta**
**Date: 2025-08-02**

This document describes the algorithm implemented in `cq_wpx_prefix.py` for determining an amateur radio callsign's official CQ WPX Contest prefix.

---
### --- Revision History ---
- 2025-08-02: Initial release of this specification document.
---

## 1. Core Purpose

The script's purpose is to implement the specific and complex rules of the CQ WPX Contest to derive the correct prefix from a given callsign. It relies on pre-processed information provided by the `get_cty.py` lookup script to resolve ambiguities.

## 2. Required Inputs

The algorithm requires a row of data containing three key fields:

1.  **`Call`**: The original, raw callsign string.
2.  **`DXCCPfx`**: The resolved DXCC prefix for the location of operation (e.g., "K", "VP2V"). This can be "Unknown".
3.  **`portableid`**: The part of a portable callsign identified by `get_cty.py` as the location designator (e.g., "7", "VP2V"). This is blank for non-portable calls.

## 3. The Lookup Algorithm

The algorithm follows a strict hierarchy. A successful determination at any step concludes the process for that callsign.

### Step 1: Initial Cleanup
The first step is to clean the raw `Call` string to remove common suffixes that are not part of a WPX prefix or a portable designator. This includes stripping suffixes like `/P`, `/M`, `/QRP` and any characters following a hyphen (`-`).

### Step 2: Maritime Mobile Override
The highest-priority rule is for maritime mobile stations. If the raw `Call` string ends in `/MM`, the WPX prefix is immediately determined to be **"Unknown"**.

### Step 3: Portable Call Processing
If the `portableid` field is not blank, the portable prefix rules are applied.

1.  **Prerequisite Check:** If the `DXCCPfx` is "Unknown," the program cannot safely resolve a portable call, and the WPX prefix is **"Unknown"**.
2.  **`call/digit` Rule:** If the `portableid` is a single digit (e.g., "7" from `WN5N/7`), a special transformation is applied: the last digit of the *root callsign's prefix* is replaced with the `portableid` digit. (e.g., `WN5N`'s prefix `WN5` becomes `WN7`).
3.  **Letters-Only Rule:** If the `portableid` contains only letters and no numbers (e.g., "LX" from `LX/KD4D`), a zero (`Ø`) is appended to it to form the prefix (`LXØ`).
4.  **Default Portable Rule:** In all other portable cases (e.g., the `portableid` is a full prefix like "VP2V"), the `portableid` itself becomes the WPX prefix.

### Step 4: Non-Portable Call Processing
If the `portableid` field is blank, the non-portable rules are applied to the cleaned callsign.

1.  **`DXCCPfx` Override Rule:** The program first checks if the callsign starts with the provided `DXCCPfx`. If it does, the `DXCCPfx` is the definitive WPX prefix. This handles special cases like `VP2VMM` (whose `DXCCPfx` is `VP2V`).
2.  **Default Rules:** If the override rule does not apply (either because the call doesn't match or the `DXCCPfx` is "Unknown"), the program uses the following fallback logic:
    * **No-Number Calls:** If the callsign has no numbers (e.g., `RAEM`), the prefix is the first two letters plus a zero (`RAØ`).
    * **Standard Calls:** The prefix is everything in the callsign up to and including the last digit (e.g., `S55A` becomes `S55`).

